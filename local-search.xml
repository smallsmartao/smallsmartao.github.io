<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>INTERNAL LANGUAGE MODEL ESTIMATION FOR DOMAIN-ADAPTIVE END-TO-END SPEECH RECOGNITION</title>
    <link href="/2022/07/01/ilme/"/>
    <url>/2022/07/01/ilme/</url>
    
    <content type="html"><![CDATA[<p>作者:Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, Yifan Gong<br>机构:  Microsoft Corporation, Redmond, WA, USA arXiv:2011.01991v1<br>链接:   <a href="https://arxiv.org/abs/2011.01991">https://arxiv.org/abs/2011.01991</a>      </p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>通过引入外部语言模型来实现adapt，提出一个不限制e2e模型的集成外部语言模型方式</p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="Shallow-Fusion"><a href="#Shallow-Fusion" class="headerlink" title="Shallow Fusion"></a>Shallow Fusion</h4><p>在beam search时引入外部语言模型，虽然后面又提出deep fusion，cold fusion，Simple Fusion and Component Fusion等方法，但是这些方法引入额外的计算代价和训练步骤，因此没有取代sf。<br><img src="https://s2.loli.net/2022/07/01/Ers2CXivb3acA8l.png" alt="sf">       </p><h4 id="Density-Ratio-Method-In"><a href="#Density-Ratio-Method-In" class="headerlink" title="Density Ratio Method In"></a>Density Ratio Method In</h4><p>类似于hybrid模型，将e2e的后验按照声学和语言分解<br><img src="https://s2.loli.net/2022/07/01/YVfHzTXE6NvjCak.png" alt="source"><br><img src="https://s2.loli.net/2022/07/01/HAC9qIuBLrc6XJp.png" alt="target"><br>我们认为声学部分两边是一样的，就得到下面的式子：<br><img src="https://s2.loli.net/2022/07/01/aguEGKbXBJItfOW.png" alt="ratio"><br>x部分在given x的情况下对所有y不变，所以只需要考虑y部分，y部分实际是两个domain语言模型后验的比值，于是我们得出西面的fusion方式。<br><img src="https://s2.loli.net/2022/07/01/HNcb6vUMVKZCnfp.png" alt="image.png"><br>注意语言模型单独训练和e2e系统是两个模型。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>asr</tag>
      
      <tag>rnn-t</tag>
      
      <tag>lm</tag>
      
      <tag>domain adapt</tag>
      
      <tag>aed</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Memory-Efficient Training of RNN-Transducer with Sampled Softmax</title>
    <link href="/2022/07/01/rnnt-0701/"/>
    <url>/2022/07/01/rnnt-0701/</url>
    
    <content type="html"><![CDATA[<p>作者：Jaesong Lee1∗, Lukas Lee1∗, Shinji Watanabe<br>关键词：RNN-T softmax </p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>RNN-T训练由于最后的输出是TxUxV的矩阵并且要过softmax这就导致这个矩阵的所有变量及其梯度都要存储，导致大量的显存占用。之前的一些方法会采用降低最后的矩阵的尺度来降低显存的占用比如使用sylable降低V，通过降采样降低T来减少显存的占用。还有一些降低padding，预训练的策略来加速rnn-t的训练。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><ol><li>sampled softmax降低训练的显存占用<br><img src="https://s2.loli.net/2022/07/01/m4EMR9uNVkUyFpq.png"><br>只包括对齐中需要的字的后验<br>Vneg源自对剩下字的采样<br><img src="https://s2.loli.net/2022/07/01/tRVawyj2AW1kfiI.png"></li></ol><p>这样的话最终计算对齐的矩阵就减小到了TxUxsmaple-V<br>但是如果采样的方式不合理会导致对部分字的后验估计错误导致解码出现不该出现的字。<br>2. 样本单独采样<br>一般的sample-softmax一个batch共用一组正负集合，这就使得在batch较大的时候，正集合变得很大，而负集合也必须跟着扩大，无法有效的降低显存。这里采用每个样本单独的正负集合来降低现存的占用，这样即使batch-size较大，两个集合也不会过大。<br>3. 利用ctc的头来协助采样<br>采样需要一个分布，如果按照平均分布肯定是不合理，我们肯定是希望让预测错误的部分进入loss来让模型学习正确的结果，但是如果每次都进行一次解码来得到后验就加大了显存的消耗，而作为正则的ctc与rnn-t的最终输出有相关性，所以我们就使用ctc的头的后验去除正字符后作为采样的分布使用。<br>4. CTC-constrained 解码<br>由于训练的时候的后验基于ctc的分布，所以解码时候我们也要引入ctc，否则会出现很多没有被正确建模概率的字，我们将ctc头的结果选出topk个字作为解码空间进行rnn-t的解码。    </p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="数据："><a href="#数据：" class="headerlink" title="数据："></a>数据：</h4><p>LibriSpeech、Aishell-1，CSJ<br>libriseech对比不同的建模单元数<br>aishell-1使用4231     </p><h5 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h5><p><img src="https://s2.loli.net/2022/07/01/3Sdkzw54O9Zifpb.png"><br><img src="https://s2.loli.net/2022/07/01/BTWOthFCHvpPsu2.png">     </p><p><img src="https://s2.loli.net/2022/07/01/fpZTOrU5EHn3bBw.png">  </p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>asr</tag>
      
      <tag>rnnt</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PROGRESSIVE VOICE TRIGGER DETECTION Latency Control for Keyword Spotting</title>
    <link href="/2022/06/30/kws-0630/"/>
    <url>/2022/06/30/kws-0630/</url>
    
    <content type="html"><![CDATA[<p>作者: Christin Jose, Joseph Wang, Grant P. Strimel, Mohammad Omar Khursheed, Yuriy Mishchenko,<br>Brian Kulis<br>机构: Amazon Science, United States<br>关键词: kws,max-pooling<br>链接:  <a href="https://arxiv.org/pdf/2206.07261.pdf">https://arxiv.org/pdf/2206.07261.pdf</a>       </p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>maxpooling不需要用对齐模型生成label，但是会导致额外的延迟，因为模型会倾向于多看一点下文再判断，而有时候我们需要灵活的控制延迟，像celoss我们就可以移动label来控制。</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>原始的maxpooling loss 对于正样例选取正类别后验最高的一帧算loss，对于负样例则选取负类别后验最低的一帧算loss<br><img src="https://s2.loli.net/2022/06/30/F3aUJjMI4uRzBkm.png" alt="maxpooling.png"><br>本文在选取正样例的位置的时候加了一个前移的随机量，这个量服从伯努利分布，通过控制分布中为1的概率参数，来控制前移的幅度。<br><img src="https://s2.loli.net/2022/06/30/Nf8Hl96Ja4wgztE.png" alt="maxpooling2.png"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><p>cnn网络，64维 Mel-filterbank energy (LFBE) features<br><img src="https://s2.loli.net/2022/06/30/Xv6K58AUHR3CGak.png" alt="cnn.png"></p><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>准确度上b越大效果越差，0.1,0.2下和celoss性能相当<br><img src="https://s2.loli.net/2022/06/30/LkxMH2tpiXzvQ6c.png" alt="res1.png"><br>对时延的影响也符合预期<br><img src="https://s2.loli.net/2022/06/30/McBSfbCE1x8kaZ5.png" alt="res2.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>复现简单，解决实际问题。</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>kws</tag>
      
      <tag>maxpooling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pruned RNN-T for fast, memory-efficient ASR training</title>
    <link href="/2022/06/30/puned-rnnt/"/>
    <url>/2022/06/30/puned-rnnt/</url>
    
    <content type="html"><![CDATA[<p>作者:  Fangjun Kuang, Liyong Guo, Wei Kang,Long Lin, Mingshuang Luo, Zengwei Yao, Daniel Povey Xiaomi<br>机构:  Xiaomi Corp., Beijing, China<br>关键词: RNN-T, memory-efficient<br>链接:  <a href="https://arxiv.org/abs/2206.13236">https://arxiv.org/abs/2206.13236</a>       </p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>RNN-T 输出一个四维的矩阵(N,T,U,V)，N为batch size，T为时间，U为序列长度，V为词典大小，V很大的情况下，训练显存占用太大了，之前有很多减少显存占用的工作，比如去除padding，在logits上计算梯度，本文提出一种方法，用S取代U，S&lt;&lt;U，从而降低内存占用。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>rnn-t三个部分：encoder，predictor，joint network  </p><ul><li>encdoer输出(T,E)维度的矩阵，E为encoder输出维度。</li><li>predictor基于之前的非blank标签预测下一个token的分布，输出(U+1,D)维度的矩阵</li><li>joiner 组合两边的输出得到(T,U+1,V)其中L[t,u,v]代表token v在t时刻出现given y[0…u]<br>rnn-t的解码图如下：<br><img src="https://s2.loli.net/2022/06/30/4IlWAgayPXZqc3J.png" alt="decode graph"><br>我们用y代表离开这个node的概率，blank代表水平移动的概率<br><img src="https://s2.loli.net/2022/06/30/or47tnRgGOkYZSA.png"><br>α(t,u)代表看到x[0…t]输出y[0…u]的log输出，推理过程如下：<br><img src="https://s2.loli.net/2022/06/30/Bb5gYoLpvxc1eWH.png"><br>包括上一个t从u水平过来和从上一个token跳过来<br><img src="https://s2.loli.net/2022/06/30/bFtRO1g7Quz8r4X.png"><br>最终RNN-T的输出是(NTUV)和ctc和aed模型的(NTV)&#x2F;(NUV)他的消耗大得多。<br><img src="https://s2.loli.net/2022/06/30/hvYzo78GLUNkrQK.png"><br>而实际上大部分梯度接近0，中间部分的梯度比较明显，可以看到训练后模型梯度非常集中，有点类似对齐的感觉。</li></ul><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><h4 id="Trivial-joiner-network"><a href="#Trivial-joiner-network" class="headerlink" title="Trivial joiner network"></a>Trivial joiner network</h4><p>避免引入一个很大线性层<br><img src="https://s2.loli.net/2022/06/30/cKFPTlHtoWOZwyq.png"></p><h4 id="Pruning-bounds"><a href="#Pruning-bounds" class="headerlink" title="Pruning bounds"></a>Pruning bounds</h4><p>引入常量S，只在S范围内计算loss，S之外的部分置为负无穷。这就需要我们找到一个S的边界，使得这个边界内的后验是最大的，本文提出一种估计方法，来估计S的起始位置。<br>设S&#x3D;4，起始为2，那么估计为<br><img src="https://s2.loli.net/2022/06/30/bGyI5fmTt762EBq.png"><br>我们需要找到其中最大的起始位置：<br><img src="https://s2.loli.net/2022/06/30/HegPh1UVbO72E6Q.png"><br>孤立着看很难理解，但是如果连续看T就能理解了。<br>为了裁剪的合理，进一步约束了起始点的位置，不能倒退，不能一步超过S。<br><img src="https://s2.loli.net/2022/06/30/DCIU6Ac4ouvWwHj.png"></p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="https://s2.loli.net/2022/06/30/YoK7lCyx1PzhdVL.png"><br><img src="https://s2.loli.net/2022/06/30/cqprab2XtOSzHNo.png"><br><img src="https://s2.loli.net/2022/06/30/G9TIZL2eXHjvScE.png"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p>比较了性能和训练效率两个方面。<br><img src="https://s2.loli.net/2022/06/30/wg7Z3etybuCPlin.png"><br><img src="https://s2.loli.net/2022/06/30/exbscahmIDEiRwT.png"><br><img src="https://s2.loli.net/2022/06/30/HFul9zKdeqQjWN1.png">   </p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>asr</tag>
      
      <tag>rnn-t</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Improving CTC-based ASR Models with Gated Interlayer Collaboration</title>
    <link href="/2022/06/30/ctc-0630/"/>
    <url>/2022/06/30/ctc-0630/</url>
    
    <content type="html"><![CDATA[<p>作者：Yuting Yang, Yuke Li, Binbin Du<br>机构：网易<br>关键词：CTC、流式、文本融合           </p><h3 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h3><p>ctc的独立性假设不合理，无法在模型前向中利用到文本信息</p><h3 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h3><p>encoder使用conformer<br>提出一种融合方法gated interlayer collaboration （GIC）<br>首先将隐状态过一个线性层映射到字典维度得到后验，之后过embedding得到textual feature     </p><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1653620245806/9-pCZOsHA.png"></p><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1653620256457/qabF0GuNs.png"></p><p>通过门控的方式融合两个模态</p><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1653620298741/K8ecK9ker.png"></p><p>最终loss是原本的ctcloss加上所有的中间CTC的平均值</p><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1653620353727/4jlpuJez_.png"></p><h3 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h3><h4 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h4><ul><li>实验数据<br>aishell-1 数据堂</li></ul><h4 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h4><p><img src="https://cdn.hashnode.com/res/hashnode/image/upload/v1653627039503/eTfIiAjja.png"></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>其实跟<br><a href="https://arxiv.org/pdf/2104.02724.pdf">Relaxing the Conditional Independence Assumption of CTC-based ASR<br>by Conditioning on Intermediate Predictions</a><br>相比就是加了一个embedding和加权相加</p>]]></content>
    
    
    <categories>
      
      <category>论文解读</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ctc</tag>
      
      <tag>asr</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
